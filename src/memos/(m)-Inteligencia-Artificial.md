## [[ciclo-de-vida-de-un-modelo-de-IA-generativa]]
#memo 

Menciona las 4 fases de un ciclo de vida de un modelo de IA generativa (en inglés) :: Scope, Select, Adapt/Align, App Integration <!--SR:!2024-03-28,3,150-->

## [[mecanismo-de-atención-artificial]]

¿Cuáles son los tres componentes de un mecanismo de atención artificial? :: consulta, claves, valores <!--SR:!2024-05-10,46,230-->

¿Cómo se calcula la atención artificial (usando sus 3 componentes)? ::Cada clave se compara en relación con la importancia que tiene para la consulta y las puntuaciones más altas pesan los valores <!--SR:!2024-03-28,3,150-->

## [[modelo-de-IA-generativa]]

¿qué es un modelo de AI generativa? :: son modelos capaces de generar información a partir de prompts <!--SR:!2024-07-17,114,230-->

¿qué técnicas o métodos se usan para entrenar un modelo de IA generativa? :: aprendizaje profundo <!--SR:!2024-04-05,69,250-->

## [[red-neuronal-artificial]]

La relación entre neuronas se da gracias a una ==función de activación==. <!--SR:!2024-04-07,13,221-->

Una neurona es una función que calcula una ==suma ponderada de las entradas== seguida de la aplicación de una función no lineal. <!--SR:!2024-03-27,1,130-->

Las redes neuronales convulsionales se usan para resolver tareas de ==visión artificial==, ya que son aplicadas en ==matrices bidimensionales== <!--SR:!2024-03-28,43,281!2024-05-05,41,292-->

Las redes neuronales recurrentes se caracterizan por ==recibir sus datos de salida como datos de entrada== y son buenas para procesar ==datos secuenciales== de forma eficiente <!--SR:!2024-04-10,16,241!2024-04-01,7,232-->

## [[transformadores-(redes-neuronales)]]

La codificación posicional permite la ==paralelización del proceso de entrenamiento== y, por lo tanto, optimiza los recursos y resultados del modelo. <!--SR:!2024-04-15,21,210-->

Los transformadores usan un mecanismo de ==atención artificial== y una estrategia llamada (en inglés) ==positional encoding== <!--SR:!2024-04-03,67,250!2024-08-21,149,312-->

La codificación posicional, en el contexto de procesamiento de texto, permite ==conocer la posición de cada palabra en su secuencia original== <!--SR:!2024-04-05,11,190-->

Los transformadores son una :: arquitectura de [[red-neuronal-artificial]] basada en el aprendizaje semi-supervisado <!--SR:!2024-03-29,4,186-->

## [[tres-arquitecturas-de-los-transformadores]]

¿Cuáles son las 3 arquitecturas de una red transformadora según sus dos componentes principales? :: encoder-only, decoder-only, encoder-decoder <!--SR:!2024-04-30,94,270-->

Al modelo *encoder-only* se le llama ==autoencoding== <!--SR:!2024-07-15,112,230-->

Al modelo *decoder-only* se le llama ==autoregresive== <!--SR:!2024-04-20,26,190-->

Al modelo *encoder-decoder* se le llama ==sequence to sequence== <!--SR:!2024-05-22,58,230-->