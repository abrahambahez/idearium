# problema del psicologismo
Muchos investigadores han evitado el psicologismo como estrategia de justificación teórica. Desafortunadamente, muchos periodistas y medios han hecho lo contrario, abusando acríticamente de los *estudios publicados por la universidad (prestigiosa o perteneciente a una potencia mundial*) que demuestran algo difícil de replicar/reproducir.

@reagle2019 [cap. 4, sec. The Science of Motivation] escribe:

>a number of efforts are being made to reexamine well-regarded and classic studies. The Reproducibility Project, a collaboration of hundreds of researchers, repeated one hundred studies from top-tier psychology journals and managed to reproduce the results of just thirty-five. Another approach, the Replicability-Index, uses meta-analysis to combine existing studies’ data so as to reveal spurious findings. This happens to include Roy Baumeister’s famed ego-depletion theory of motivation, though Baumeister remains convinced his methods and conclusions about blood-sugar are sound. ([[agotamiento-del-ego]])
>
>The reformers believe that researchers have had too much freedom to steer work toward statistically significant results. A _p-_value represents the likelihood that a novel finding would happen by chance alone—5 percent is a typical upper threshold for significance. Yet this value is meaningful only in the context of testing a single hypothesis, _not_ of fishing for results across many variables. For example, the chocolate study took eighteen measurements from fifteen people: blood protein levels, cholesterol, sleep quality, weight, well-being, etc. By trawling for results across eighteen measurements, the researchers had a 60 percent chance of finding something that looked significant at the 5 percent level. Whether researchers inadvertently fool themselves or purposefully trick their peers, the practice is known as _p-hacking_, appropriately enough.
>
>The problem of p-hacking ([[inferencia-selectiva-p-hacking]]) is compounded by journals’ publishing only novel findings. For every remarkable result (e.g., “chocolate accelerates weight loss”), there are many unpublished studies that find the opposite but are never published. This publication bias distorts our understanding of the world and contributes to baseless self-help advice.
>
>One way forward is for researchers to register their hypotheses and methods before any data are collected. This limits researchers’ degrees of freedom in shaping the analysis. Additionally, even negative findings should be published so as to prevent publication bias.