# arquitectura encoder-only
Una de las [[tres-arquitecturas-de-los-transformadores]]. *Encoder only* es tambi√©n llamada *Autoencoding* y *Masked Language Modeling*.

Ejemplos de esta arquitectura son los modelos como Bert y Roberta, enmascara aleatoriamente *tokens* para ser reconstruidos con un contexto bidireccional

En el contexto de [[procesamiento-de-lenguaje-natural]], esta arquitectura es buena para las siguientes tareas:

- [[clasificacion-de-sentimiento]]
- [[reconocimiento-de-entidades-nombradas]]
- [[clasificacion-de-secuencias]]