# A Brief Introduction to Generative Models

Alex Lamb (2021), [zotero](zotero://select/items/@lamb2021) , [archivo](file:///home/sabhz/archivo/librero/lamb2021.pdf)

## Resumen

We introduce and motivate generative modeling as a central task for machine learning and provide a critical view of the algorithms which have been proposed for solving this task. We overview how generative modeling can be defined mathematically as trying to make an estimating distribution the same as an unknown ground truth distribution. This can then be quantified in terms of the value of a statistical divergence between the two distributions. We outline the maximum likelihood approach and how it can be interpreted as minimizing KL-divergence. We explore a number of approaches in the maximum likelihood family, while discussing their limitations. Finally, we explore the alternative adversarial approach which involves studying the differences between an estimating distribution and a real data distribution. We discuss how this approach can give rise to new divergences and methods that are necessary to make adversarial learning successful. We also discuss new evaluation metrics which are required by the adversarial approach. [modelo-de-IA-generativa](modelo-de-IA-generativa.md)

## Notas

# Anotaciones

(17/10/2023 8:57:56)

"Suppose we have a dataset where a model generates text captions from an image. Consider an image of a giraffe, which the model describes as “A giraffe walking next to tall green grass”. It’s possible that the model has learned enough to be able to recognize that there is a giraffe, and that the giraffe is walking (and not running or sitting), and that there is tall green grass. However, another possibility is that the model recognizes the giraffe, but simply says that the giraffe is walking because giraffes are usually walking, and says that the giraffe is near tall grass because giraffes are usually near tall grass, and says that the grass is green because grass is usually green. Thus it’s difficult to know if the model really understands the image, or if it’s merely making reasonable guesses based on what types of images are common." (Lamb, 2021, p. 2)

"Since humans can generate arbitrary text by hand, we could easily supply the model with counterfactuals like “A running giraffe next to short red grass” or “A giraffe lying down next to tall blue grass”. Since humans easily generate detailed counterfactuals in text, it would be easy to verify how well the model understands the world." (Lamb, 2021, p. 2)

"The essential idea is that we treat observations from the world as samples from a distribution x ∼ p(x)." (Lamb, 2021, p. 2)

"At the same time, we can interpret our generative model as an estimating distribution qθ(x), which is described by a set of parameters θ. Then we can frame generative modeling as trying to ensure that p(x) and qθ(x) become as similar as possible. Statistical divergences give a natural mathematical framework for this." (Lamb, 2021, p. 3)

"The Likelihood Maximization Approach" (Lamb, 2021, p. 4)

"What is the right algorithm for finding a distribution qθ(x) which minimizes a divergence between itself and p(x)." (Lamb, 2021, p. 4)

"Energy-Based Models" (Lamb, 2021, p. 7)

"Autoregressive Models" (Lamb, 2021, p. 10)

"Variational Autoencoders" (Lamb, 2021, p. 12)

"The Adversarial Approach" (Lamb, 2021, p. 14)

"Generative Adversarial Networks" (Lamb, 2021, p. 15)

"Wasserstein GAN" (Lamb, 2021, p. 17)

"Spectral Normalization" (Lamb, 2021, p. 18)

"Jacobian Clamping" (Lamb, 2021, p. 19)
