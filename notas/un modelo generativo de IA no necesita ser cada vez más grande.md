# un modelo generativo de IA no necesita ser cada vez más grande
Inicialmente, se tenía la idea de que la calidad en los resultados de [[modelo de IA generativa]] debería ir creciendo con el tiempo en: 

1. Número de parámetros
2. Tamaño de los datos entrenamiento y,
3. Recursos computacionales para correrlo.

Pero esto no es necesariamente cierto, ya que la relación entre estas tres condiciones puede optimizarse o producir resultados negativos. Un modelo puede estar *sobre-parametrizado* o *sub-entrenado* si el tamaño de los datos de entrenamiento o el número de parámetros no se equilibran (y otros problemas similares).